{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e5fdd9a3",
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidIndexError",
     "evalue": "Reindexing only valid with uniquely valued Index objects",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidIndexError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-43-2a9a4364d421>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[0mtemp_df3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# copies content of CSV file into 'temp_df3' pandas DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     75\u001b[0m             \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# deletes CSV file\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 76\u001b[1;33m             \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdf3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtemp_df3\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# adds data from 'temp_df3' pandas DataFrames to 'df3' DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     77\u001b[0m             \u001b[0mdf3\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdrop_duplicates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msubset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'dupe_check'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# deletes duplicate entries in 'long_date' column of 'df3' DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     78\u001b[0m         \u001b[0mdf3\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_file_path\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# creates CSV file and paste all data from DataFrame\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mconcat\u001b[1;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[0;32m    296\u001b[0m     )\n\u001b[0;32m    297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 298\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    299\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    300\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\reshape\\concat.py\u001b[0m in \u001b[0;36mget_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    514\u001b[0m                     \u001b[0mobj_labels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    515\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnew_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequals\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 516\u001b[1;33m                         \u001b[0mindexers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0max\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mobj_labels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    518\u001b[0m                 \u001b[0mmgrs_indexers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindexers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mget_indexer\u001b[1;34m(self, target, method, limit, tolerance)\u001b[0m\n\u001b[0;32m   3169\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3170\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3171\u001b[1;33m             raise InvalidIndexError(\n\u001b[0m\u001b[0;32m   3172\u001b[0m                 \u001b[1;34m\"Reindexing only valid with uniquely valued Index objects\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3173\u001b[0m             )\n",
      "\u001b[1;31mInvalidIndexError\u001b[0m: Reindexing only valid with uniquely valued Index objects"
     ]
    }
   ],
   "source": [
    "# This python script imports quarter-hourly powergen data for Germany from https://transparency.entsoe.eu using RESTful API\n",
    "# more detailed instructions on how to use API are on https://github.com/EnergieID/entsoe-py\n",
    "# mappings of all available parameters are in 'Appendix A' on https://transparency.entsoe.eu/content/static_content/Static%20content/web%20api/Guide.html#_generation_domain AND also on https://github.com/EnergieID/entsoe-py\n",
    "# Maximum of 400 requests per user per minute. Reaching of 400 query/minute limit will result in a temporary ban of 10 minutes.\n",
    "\n",
    "from entsoe import EntsoePandasClient # to pull Entso-E data thru API\n",
    "import pandas as pd\n",
    "import os # to check if csv file already exists before exporting the data into csv. Also to delete CSV file \n",
    "from dotenv import load_dotenv # I need this to hide API key\n",
    "import os # I need this to hide API key\n",
    "\n",
    "# I need this to hide API key\n",
    "load_dotenv()\n",
    "client = EntsoePandasClient(api_key=os.getenv('api_key')) # enters my API token\n",
    "\n",
    "start = pd.Timestamp('20211227', tz='Europe/Brussels') # start date\n",
    "end = pd.Timestamp('20211228', tz='Europe/Brussels') # end date\n",
    "\n",
    "country_codes = ['DE_LU', 'DK']\n",
    "#country_codes = ['AT', 'BE', 'CZ', 'DE', 'DE_LU', 'DK', 'ES', 'FR', 'HR', 'HU', 'IE', 'IT', 'LT', 'ME', 'MK', 'NL', 'PT', 'SE', 'SK', 'UA', 'AL', 'BA', 'BG', 'BY', 'CH', 'DK', 'EE', 'FI', 'GB', 'GB_NIR', 'GR', 'LU', 'LV', 'MD', 'MT', 'NO', 'PL', 'RO', 'RS', 'RU', 'RU_KGD', 'SI', 'TR', 'DE_AT_LU']\n",
    "\n",
    "data_types = ['powergen', 'consumption'] # create list for data types in Entso-E's database (i.e. power generation, power consumption, power capacity). If you add new data type here, make sure you add it to 'if data_type...' statement below\n",
    "# BUG!!!!\n",
    "# unable to add 'capacity' to 'data_types = ['powergen', 'consumption']' because it triggers an error if 'Entso-E_capacity_per_country_API.csv' file already exists because of the  bug in 'df3 = pd.concat([df3, temp_df3])' line below\n",
    "for data_type in data_types: # loops through types of data in Entso-E's database (i.e. power generation, power consumption, power capacity) to determine which type of API query and CSV file name to use \n",
    "    for country_code in country_codes: # loops through countries in 'country_codes' list\n",
    "        \n",
    "        # clears out DataFrames\n",
    "        df1 = df1[0:0]\n",
    "        df2 = df2[0:0]\n",
    "        df3 = df3[0:0]\n",
    "        temp_df3 = temp_df3[0:0]\n",
    "        \n",
    "        # get data from API for each data type in 'data_types' list above\n",
    "        if data_type == \"powergen\":\n",
    "            df1 = client.query_generation(country_code, start=start,end=end) # get data for power generation and store in 'df1' DataFrame\n",
    "            csv_file_path = '/Users/elchi/Dropbox/Coding/Entso-E_powergen_per_country_API.csv' # create file path to store data for power generation\n",
    "        elif data_type == \"capacity\":\n",
    "            df1 = client.query_installed_generation_capacity(country_code, start=start,end=end) # get data for power capacity and store in 'df1' DataFrame\n",
    "            csv_file_path = '/Users/elchi/Dropbox/Coding/Entso-E_capacity_per_country_API.csv' # create file path to store data for power capacity\n",
    "        elif data_type == 'consumption':\n",
    "            df1 = client.query_load(country_code, start=start,end=end) # get data for power consumption and store in 'df1' DataFrame\n",
    "            csv_file_path = '/Users/elchi/Dropbox/Coding/Entso-E_consumption_per_country_API.csv' # create file path to store data for power consumption\n",
    "        else:\n",
    "            print('not recognised')\n",
    "\n",
    "        df1.reset_index(inplace=True) # remove time & date stored as index value (i.e. row header) by resetting index values and storing time and date under new 'Time and Date' column\n",
    "        df1 = df1.rename(columns={'index': 'Time and Date'})\n",
    "\n",
    "        # BUG!!!!\n",
    "        # below section triggers an error if certain country codes (AL, DK, etc) are used\n",
    "        # Within multi-level columns in 'df1' DataFrame for 'consumption' data set, search for lower columns (i.e. columns in second row) titled \"Actual Consumption\", then for the upper column (i.e. columns in first row) copy the name of the column to the left of it and add 'Consumption' to the end of that column title. Then, delete all lower columns (i.e. second-row column).\n",
    "        if data_type == 'powergen':\n",
    "            col_list = [] # creates empty list to store column values\n",
    "            for col_num in range(0, len(df1.columns)): # loops through columns in 'df1' DataFrame\n",
    "                if df1.columns[col_num][1] == \"Actual Consumption\": # finds lower columns who's name equal 'Actual Consumption'\n",
    "                    col_list.append(df1.columns[col_num-1][0] + \" Consumption\") # adds name of upper column to the left of the matching column (i.e. the one that says 'Actual Consumption') and suffix 'Consumption' to 'col_list'\n",
    "                else:\n",
    "                    col_list.append(df1.columns[col_num][0]) # adds name of column to 'col_list'\n",
    "            df1.columns = col_list # renames columns in 'df1' DataFrame using names set in 'col_list' and deletes lower collumns (i.e. columns in second row)\n",
    "\n",
    "        # Create 'df2' DataFrame and populate it with single 'Country' column and set row values to country code used in 'country_code' variable \n",
    "        country_val = [] # will store values which will be used to add country code to df2 DataFrame \n",
    "        dupe_check = [] # will store strings which will be used to remove duplicates\n",
    "\n",
    "        for cntry in range(0, len(df1.index)): # populates 'df2' DataFrame with country code under 'Country' column and adds 'dupe_check' column which will be later used to remove duplicate entries when exporting data to Excel\n",
    "            country_val.append(country_code)\n",
    "            dupe_check.append(str(df1.iloc[cntry][0]) + \" \" + str(country_code)) # combines values for from 'date_quarterhour', '' and '' lists into a new 'dupe_check' list\n",
    "        df2 = pd.DataFrame({'dupe_check':dupe_check, 'Country':country_val})\n",
    "\n",
    "        # merge 'df1' and 'df2' into a single 'df3' DataFrame\n",
    "        df3 = df2.join(df1, how='left')\n",
    "        df3 # display DataFrame\n",
    "\n",
    "        # BUG!!!!\n",
    "        # below code doesn't keep data in right columns for 'powergen' dataset. The problem may be in this line of code above: 'df1 = df1.rename(columns={'index': 'Time and Date'})'\n",
    "        # Export DataFrame to CSV file and remove duplicate entries\n",
    "        file_exists = os.path.exists(csv_file_path) # to check if csv file already exists before exporting the data into csv. Returns True if the file exists, False if the file doesn't exist.\n",
    "        if file_exists: # if CSV file already exists, adds content of that CSV file to existing 'df3' pandas DataFrame, then deletes CSV file, drops duplicate entries in 'df3' DataFrame\n",
    "            temp_df3 = pd.read_csv(csv_file_path) # copies content of CSV file into 'temp_df3' pandas DataFrame\n",
    "            os.remove(csv_file_path) # deletes CSV file\n",
    "            df3 = pd.concat([df3, temp_df3]) # adds data from 'temp_df3' pandas DataFrames to 'df3' DataFrame \n",
    "            df3 = df3.drop_duplicates(subset='dupe_check') # deletes duplicate entries in 'long_date' column of 'df3' DataFrame \n",
    "        df3.to_csv(csv_file_path, index=False) # creates CSV file and paste all data from DataFrame\n",
    "                \n",
    "# # get data for \"A72 - Reservoir filling information\"\n",
    "\n",
    "# CREATE NEW DATAFRAMES AND CSV FILES WITH DATA AGGREGATED TO MONTHLY & YEARLY VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b202cadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "powergen db\n",
      "capacity db\n",
      "consumption db\n"
     ]
    }
   ],
   "source": [
    "def check_function(x): # create functions to loop through types of data in Entso-E's database (i.e. power generation, power consumption, power capacity)\n",
    "    if x == \"powergen\":\n",
    "        print('powergen db')\n",
    "    elif x == \"capacity\":\n",
    "        print('capacity db')\n",
    "    elif x == ('consumption'):\n",
    "        print('consumption db')\n",
    "    else:\n",
    "        print('not recognised')\n",
    "        \n",
    "data_types = ['powergen', 'capacity', 'consumption'] # create list for data types in Entso-E's database (i.e. power generation, power consumption, power capacity)\n",
    "for data_type in data_types: # loops through types of data in Entso-E's database (i.e. power generation, power consumption, power capacity)\n",
    "    check_function(data_type)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0 (tags/v3.10.0:b494f59, Oct  4 2021, 19:00:18) [MSC v.1929 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "369f2c481f4da34e4445cda3fffd2e751bd1c4d706f27375911949ba6bb62e1c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
